{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I duplicate comments that occur below:\n",
    "you can find list of used names in repo by datasets/partial_datasets/confusing_names.csv\n",
    "you can find generated results in repo by datasets/partial_datasets/confusing_generated.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's code from post below, i played with loading in 4bits and 8bits and stayed with 4,\n",
    "wrote custom prompt after like 5 tries and manual checks,\n",
    "didn't come up with any stategy that will be suitable for easy\n",
    "annotation except of wrapping specific elevation in ||,\n",
    "if elevation name doesn't match with the name i provided i don't label it as a mountain,\n",
    "i label it as elevation, i'm not sure if it's right choice but after extraction of data from\n",
    "geonames i got an idea that 'mountain' and moutai ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers pydantic bitsandbytes-cuda110 bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "model_dir = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2/\"\n",
    "\n",
    "# after manually comparing results of 4b vs 8b\n",
    "# I chose to stay with 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Adjust model configuration\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load CSV files\n",
    "# you can find it in repo by datasets/partial_datasets/\n",
    "# i think generating more such confusing names could improve performance in case of outliers like this\n",
    "confusing_names_df = pd.read_csv(\"/kaggle/input/confusing-mount-names/confusing_names.csv\")\n",
    "\n",
    "confusing_name_samples = list(confusing_names_df[\"Mountain Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an advanced AI trained in natural language processing and synthetic data generation.\n",
    "Your task is to read the following mountain name and generate 5 unique sentences using mountain name in context that sentence won't be about mountain.\n",
    "After that generate 3 sentences about mountain using given mountain name that could be understood from context - don't mention words mountain or mount but you can mention mountain name.\n",
    "After that 2 sentences about mountain using given mountain name. \n",
    "For each mountain name, provide a JSON output with the following structure:\n",
    "- \"sentences_not_about_mountain\": [ array that contains 5 sentences which are strings]\n",
    "- \"sentences_about_mountain_implied_from_context\": [ array that contains 3 sentences which are strings]\n",
    "- \"sentences_directly_about_mountain\": [ array that contains 2 sentences which are strings]\n",
    "\n",
    "Make sure to extract the exact string of the mountain name without any corrections or conversions.\n",
    "For sentences about mountain implied from context and sentences directly about mountain highlight name of peak, mountain or range by setting \"||\" around it. Do not provide any explanations.\n",
    "Only respond with the JSON structured data.\n",
    "\n",
    "### Example 1:\n",
    "Input:\n",
    "'\n",
    "Mount Adams\n",
    "'\n",
    "\n",
    "Output:\n",
    "[\n",
    "    {\n",
    "        \"sentences_not_about_mountain\": [\n",
    "            \"Adams was thrilled to receive the promotion she had worked so hard for.\",\n",
    "            \"The Adams family has been living in this town for generations, contributing to its growth.\",\n",
    "            \"During the lecture, Professor Adams captivated the audience with her groundbreaking research.\",\n",
    "            \"The bakery on Main Street is famous for its delicious Adams Apple Pie recipe.\",\n",
    "            \"Adams signed the treaty, marking a significant turning point in the negotiations.\"\n",
    "        ],\n",
    "        \"sentences_about_mountain_implied_from_context\": [\n",
    "            \"||Adams|| stood tall in the distance, its silhouette painted against the twilight sky.\",\n",
    "            \"The hikers marveled at the breathtaking view from ||Adams||, feeling a sense of accomplishment.\",\n",
    "            \"At sunrise, the peak of ||Adams|| glowed with a warm golden hue, mesmerizing everyone at the base.\"\n",
    "        ],\n",
    "        \"sentences_directly_about_mountain\": [\n",
    "            \"||Mount Adams|| is a favorite destination for those seeking adventure and tranquility in nature.\",\n",
    "            \"Legends say that ||Mount Adams|| has been a source of inspiration for poets and explorers alike.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "### Example 2:\n",
    "Input:\n",
    "'\n",
    "Mount Wilson\n",
    "'\n",
    "\n",
    "Output:\n",
    "[\n",
    "    {\n",
    "      \"sentences_not_about_mountain\": [\n",
    "        \"Wilson greeted everyone warmly at the family reunion, his smile lighting up the room.\",\n",
    "        \"The town's new mayor, Wilson, promised to bring change and prosperity to the community.\",\n",
    "        \"As the storm intensified, Wilson tightened his grip on the steering wheel, determined to get everyone home safely.\",\n",
    "        \"Wilson was the lead detective on the case, tirelessly piecing together clues to solve the mystery.\",\n",
    "        \"Her dog, Wilson, wagged his tail excitedly as she prepared his dinner.\"\n",
    "      ],\n",
    "      \"sentences_about_mountain_implied_from_context\": [\n",
    "        \"The sunrise over ||Wilson|| painted the horizon in hues of orange and gold, leaving the hikers speechless.\",\n",
    "        \"Legends say that ||Wilson|| holds secrets from ancient times, guarded by the winds and stars.\",\n",
    "        \"The view from the top of ||Wilson|| stretched endlessly, revealing a patchwork of forests and shimmering lakes.\"\n",
    "      ],\n",
    "      \"sentences_directly_about_mountain\": [\n",
    "        \"||Mount Wilson|| is known for its challenging trails that attract adventurers from around the world.\",\n",
    "        \"The icy winds sweeping through ||Mount Wilson|| make it a formidable yet mesmerizing destination.\"\n",
    "      ]\n",
    "    }\n",
    "]\n",
    "\n",
    "Please continue with task and stop only after generating valid output for given mountain by user by outputting '### Output ends here.' don't forget this strict rule, strictly output valid JSON structure, never forget [] brackets. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "import os\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# extracting needed json structure based on output structure defined by prompt\n",
    "def extract_json_from_response(response):\n",
    "    try:\n",
    "        response = response[response.find(\"User:\"):]\n",
    "        response = response[:response.find(\"### Output ends here.\")]\n",
    "        actual_jsonlike = response[response.find(\"[\"):]\n",
    "        match = re.search(r'(\\[.*)', actual_jsonlike, re.DOTALL)\n",
    "        if match:\n",
    "            json_content = match.group(0).strip()\n",
    "            if not json_content.endswith(']'):\n",
    "                json_content += ']'\n",
    "            return json.loads(json_content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# I left this function but basically it's not doing it's job\n",
    "# no matter what combinations i tried\n",
    "# i couldn't figure out how to write function that will work, \n",
    "def create_stopping_criteria(stop_words, tokenizer, device):\n",
    "    class StoppingCriteriaSub(StoppingCriteria):\n",
    "        def __init__(self, stops = [], device=device, encounters = 1):\n",
    "            super().__init__()\n",
    "            self.stops = stops = [stop.to(device) for stop in stops]\n",
    "\n",
    "        def __call__(self, input_ids: LongTensor, scores: FloatTensor) -> bool:\n",
    "            last_token = input_ids[0][-1]\n",
    "            for stop in self.stops:\n",
    "                if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    stop_word_ids = [tokenizer(stop_word,\n",
    "                               return_tensors=\"pt\", \n",
    "                               add_special_tokens=False)[\"input_ids\"].squeeze() \n",
    "                               for stop_word in stop_words]\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_word_ids)])\n",
    "    return stopping_criteria\n",
    "\n",
    "\n",
    "stop_words_list = [\"Output ends\"]\n",
    "stopping_criteria = None\n",
    "if stop_words_list is not None:\n",
    "    stopping_criteria = create_stopping_criteria(stop_words_list, tokenizer, device)\n",
    "\n",
    "def write_batch_to_json(batch_entities, start_idx):\n",
    "    filename = os.path.join(\"/kaggle/working/generated/\", \"batch_\" + str(start_idx) + \".json\")\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(batch_entities, f)\n",
    "        \n",
    "\n",
    "def predict_entities_in_batches(test_dataset, model, tokenizer, system_prompt, batch_size=8):\n",
    "    text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Processing batches\"):\n",
    "            batch_entities = []\n",
    "            if (i + batch_size) > len(test_dataset):\n",
    "                batch_prompts = test_dataset[i:]\n",
    "            else:\n",
    "                batch_prompts = test_dataset[i:i + batch_size]\n",
    "\n",
    "            chat_inputs = [\n",
    "                f\"{system_prompt}\\nUser: {prompt}\" for prompt in batch_prompts\n",
    "            ]\n",
    "            results = text_generation_pipeline(chat_inputs, max_new_tokens=400, do_sample=False, stopping_criteria=stopping_criteria)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            for result in results:\n",
    "                generated_text = result[0]['generated_text']\n",
    "                entities = extract_json_from_response(generated_text)\n",
    "                batch_entities.append(entities)\n",
    "            \n",
    "            write_batch_to_json(batch_entities, i) \n",
    "\n",
    "os.makedirs(\"/kaggle/working/generated/\", exist_ok=True)\n",
    "dataset = confusing_name_samples\n",
    "processed_data = predict_entities_in_batches(dataset, model, tokenizer, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/generated', \"zip\", '/kaggle/working/generated/')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6242286,
     "sourceId": 10117313,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6249299,
     "sourceId": 10126787,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
