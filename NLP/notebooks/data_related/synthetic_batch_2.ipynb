{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting temperature=1.1, top_p=0.9 as expected - caused a lot of structurally incorrect outputs\n",
    "after filtering in dataset creating left like 58% of all outputs.\n",
    "But it allowed to create sentences that was different from 1st batch structurally and by meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i will duplicate same as in report - if I  would start over I would create 1 notebook in kaggle,\n",
    "create .py files in repo and will create clean notebooks using repo, lesson learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I duplicate comments that occur below:\n",
    "you can find list of used names in repo by NLP/datasets/partial_datasets/random1500Mountains.csv\n",
    "you can find generated results in repo by datasets/partial_datasets/synthetic_batch_2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes-cuda110 bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import gc  # Added import for garbage collection\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Local model and tokenizer paths\n",
    "model_dir = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2/\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Adjust model configuration\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# it can be found in NLP/datasets/partial_datasets/random1500Mountains.csv\n",
    "mt_df = pd.read_csv(\"/kaggle/input/mountain-names-1500/random1500Mountains.csv\")\n",
    "\n",
    "mt_name_samples = list(mt_df[\"name\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an advanced AI trained in natural language processing and synthetic data generation. Your task is to read the following mountain name and generate 10 unique sentences using mountain name.\n",
    "Make main focus on diversifying sentences - sentences structures and words.\n",
    "\n",
    "Make sure to extract the exact string of the mountain name without any corrections or conversions.\n",
    "For each sentence highlight name of peak, mountain or range by setting \"||\" around it. Do not provide any explanations.\n",
    "Only respond with the JSON structured data, structure of JSON should be strictly as in examples.\n",
    "\n",
    "### Example 1:\n",
    "Input:\n",
    "'\n",
    "Cerro Torre\n",
    "'\n",
    "\n",
    "Output:\n",
    "[\n",
    "    {\n",
    "        \"Cerro Torre\": [\n",
    "            \"The imposing peak of ||Cerro Torre|| is one of the most challenging climbs in the ||Patagonia Range||.\",\n",
    "            \"He's accustomed to climbing on the ||Cerro Torre||.\",\n",
    "            \"Many climbers dream of scaling the icy slopes of ||Cerro Torre||, nestled within the majestic ||Patagonia Range||.\",\n",
    "            \"Explorers often describe the air around ||Cerro Torre|| as crisp and invigorating, a testament to its pristine surroundings.\",\n",
    "            \"The rugged terrain of ||Cerro Torre|| serves as a natural barrier, preserving the unique biodiversity of the region.\",\n",
    "            \"Let's go for a walk to the ||Cerro Torre||, where the world seems to stretch endlessly below.\",\n",
    "            \"The treacherous weather around ||Cerro Torre|| makes it one of the most daunting peaks in the ||Patagonia Range|| to conquer.\",\n",
    "            \"Mountain ||Cerro Torre|| is revered by mountaineers for its challenging ascents and breathtaking vistas.\",\n",
    "            \"The first light of dawn on ||Cerro Torre|| unveils a panorama of unparalleled beauty.\",\n",
    "            \"You'll see mount ||Cerro Torre|| through the window.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "### Example 2:\n",
    "Input:\n",
    "'\n",
    "Mount Foraken\n",
    "'\n",
    "\n",
    "Output:\n",
    "[\n",
    "    {\n",
    "        \"Mount Foraken\": [\n",
    "            \"The local guides often share stories of ||Mount Foraken's|| hidden trails and secluded beauty within the ||Alaska Range||.\",\n",
    "            \"||Mount Foraken|| is renowned for its serene ambiance and untouched surroundings in the heart of the ||Alaska Range||.\",\n",
    "            \"The sunrise at ||Mount Foraken|| paints the sky in shades of orange and pink, leaving travelers mesmerized by the beauty of the ||Alaska Range||.\",\n",
    "            \"The journey to ||Mount Foraken|| passes through dense forests and meadows filled with wildflowers, all nestled in the breathtaking ||Alaska Range||.\",\n",
    "            \"Many travelers consider a visit to ||Mount Foraken||to be a spiritual and refreshing escape.\",\n",
    "            \"||Mount Foraken||is a towering symbol of resilience for the native community.\",\n",
    "            \"They couldn't decide whether to go to the ||Mount Foraken||or the sea.\",\n",
    "            \"From afar, ||Mount Foraken||stands majestic, wrapped in clouds and mystery.\",\n",
    "            \"The rugged paths leading to ||Mount Foraken||are a test of endurance for adventurers.\",\n",
    "            \"Every season brings a unique charm to ||Mount Foraken||, from snowy peaks to lush greenery.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "Continue with task and stop after generating valid output for given mountain by user by outputting '### Output ends here.' don't forget this strict rule. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "import os\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "def extract_json_from_response(response):\n",
    "    try:\n",
    "        response = response[response.find(\"User:\"):]\n",
    "        response = response[:response.find(\"### Output ends here.\")]\n",
    "        actual_jsonlike = response[response.find(\"[\"):]\n",
    "        match = re.search(r'(\\[.*)', actual_jsonlike, re.DOTALL)\n",
    "        if match:\n",
    "            json_content = match.group(0).strip()\n",
    "            if not json_content.endswith(']'):\n",
    "                json_content += ']'\n",
    "            return json.loads(json_content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def create_stopping_criteria(stop_words, tokenizer, device):\n",
    "    class StoppingCriteriaSub(StoppingCriteria):\n",
    "        def __init__(self, stops = [], device=device, encounters = 1):\n",
    "            super().__init__()\n",
    "            self.stops = stops = [stop.to(device) for stop in stops]\n",
    "\n",
    "        def __call__(self, input_ids: LongTensor, scores: FloatTensor) -> bool:\n",
    "            last_token = input_ids[0][-1]\n",
    "            for stop in self.stops:\n",
    "                if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    stop_word_ids = [tokenizer(stop_word,\n",
    "                               return_tensors=\"pt\", \n",
    "                               add_special_tokens=False)[\"input_ids\"].squeeze() \n",
    "                               for stop_word in stop_words]\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_word_ids)])\n",
    "    return stopping_criteria\n",
    "\n",
    "\n",
    "stop_words_list = [\"Output ends\"]\n",
    "stopping_criteria = None\n",
    "if stop_words_list is not None:\n",
    "    stopping_criteria = create_stopping_criteria(stop_words_list, tokenizer, device)\n",
    "\n",
    "def write_batch_to_json(entities, start_idx):\n",
    "    filename = os.path.join(\"/kaggle/working/generated/\", \"entity_\" + str(start_idx) + \".json\")\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(entities, f)\n",
    "        \n",
    "\n",
    "def predict_entities_in_batches(test_dataset, model, tokenizer, system_prompt):\n",
    "    text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_dataset)), desc=\"Processing batches\"):\n",
    "\n",
    "            prompt = test_dataset[i]\n",
    "\n",
    "            chat_input = [\n",
    "                f\"{system_prompt}\\nUser: {prompt}\"\n",
    "            ]\n",
    "            results = text_generation_pipeline(chat_input,\n",
    "                                               max_new_tokens=400,\n",
    "                                               do_sample=True,\n",
    "                                               stopping_criteria=stopping_criteria,\n",
    "                                               temperature=1.1,\n",
    "                                               top_p=0.9)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            entities = []\n",
    "            for result in results:\n",
    "                generated_text = result[0]['generated_text']\n",
    "                entity = extract_json_from_response(generated_text)\n",
    "                entities.append(entity)\n",
    "            \n",
    "            write_batch_to_json(entities, i) \n",
    "\n",
    "\n",
    "os.makedirs(\"/kaggle/working/generated/\", exist_ok=True)\n",
    "# results in repo by datasets/partial_datasets/synthetic_batch_2.zip\n",
    "dataset = mt_name_samples[:1000]\n",
    "processed_data = predict_entities_in_batches(dataset, model, tokenizer, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/generated', \"zip\", '/kaggle/working/generated/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6258772,
     "sourceId": 10140589,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
